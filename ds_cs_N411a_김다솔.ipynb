{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ds_cs_N411a_김다솔.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/scarletdskim/Python-Movie-Recommendation/blob/master/ds_cs_N411a_%EA%B9%80%EB%8B%A4%EC%86%94.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CzIdLqSrQNwo"
      },
      "source": [
        "<img src='https://user-images.githubusercontent.com/6457691/90080969-0f758d00-dd47-11ea-8191-fa12fd2054a7.png' width = '200' align = 'right'>\n",
        "\n",
        "## *DATA SCIENCE / SECTION 4 / SPRINT 1 / NOTE 1 - assignmnet*\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# N411. 인공신경망(Artificial Neural Networks) 과제"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K83MOdklQlhR"
      },
      "source": [
        "## 1) 아래 만들어진 함수를 이용하여 XOR GATE를 제작해보세요. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mApkeTXiP1LA"
      },
      "source": [
        "def AND(x1, x2):\n",
        "  w1, w2, theta = 0.5, 0.5, 0.7\n",
        "  tmp = x1*w1 + x2*w2\n",
        "  if tmp <= theta:\n",
        "    return 0\n",
        "  elif tmp > theta:\n",
        "    return 1\n",
        "\n",
        "def  NAND(x1, x2):\n",
        "  w1, w2, theta = 0.5, 0.5, 0.7\n",
        "  tmp = x1*w1 + x2*w2\n",
        "  if tmp <= theta:\n",
        "    return 1 ## only change here\n",
        "  elif tmp > theta:\n",
        "    return 0 ## only change here\n",
        "\n",
        "def OR(x1, x2):\n",
        "  w1, w2, theta = 0.5, 0.5, 0.3 ## only change here\n",
        "  tmp = x1*w1 + x2*w2\n",
        "  if tmp <= theta:\n",
        "    return 0 \n",
        "  elif tmp > theta:\n",
        "    return 1 "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZWnzfQobQi4h"
      },
      "source": [
        "이 게이트는 두 입력 값 x1, x2가 다를 때만 1을 출력합니다.\n",
        "\n",
        "(Hint: 위에서 만든 함수를 종합해서(묶어서) 흐름을 만들도록 활용하면 됩니다)\n",
        "\n",
        "XOR(1, 1) = 0\n",
        "\n",
        "XOR(1, 0) = 1\n",
        "\n",
        "XOR(0, 1) = 1\n",
        "\n",
        "XOR(0, 0) = 0\n",
        "\n",
        "---\n",
        "\n",
        "### 문항 1) 논리 구조로 XOR을 표현하시오\n",
        "코딩을 통해서 확인을 하고, \n",
        "정답을 제출할 때에는 논리수식을 이용하여 제출합니다. not(~), or(|), and(&) \n",
        "\n",
        "정답 작성 형태 : 함수의 연속 사용 \n",
        "- 예1) **NAND(x1, x2) = ~AND(x1, x2)**\n",
        "- 예2) **AND(x1, x2) | OR(x1, x2)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SdvKP8tPkr1O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de36383c-4a7b-4511-8e7f-29262726940c"
      },
      "source": [
        "# 문항 1 \n",
        "#        | AND | NAND | OR | XOR\n",
        "# --------------------------------\n",
        "#  [0,0] |  0  |   1  |  0 |  0\n",
        "#  [0,1] |  0  |   1  |  1 |  1\n",
        "#  [1,0] |  0  |   1  |  1 |  1\n",
        "#  [1,1] |  0  |   0  |  1 |  0\n",
        "\n",
        "#  NAND와 OR로 나온 결과를 AND에 통과시켜주면 XOR의 결과를 얻을 수 있음\n",
        "#  XOR(x1, x2) = NAND(x1, x2) & OR(x1, x2)\n",
        "\n",
        "def XOR(x1, x2):\n",
        "  return AND(NAND(x1, x2), OR(x1, x2))\n",
        "\n",
        "print(XOR(0,0), XOR(0,1), XOR(1,0), XOR(1,1))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 1 1 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u6nSiT7MQ0AL"
      },
      "source": [
        "## 2) 신경망 용어 퀴즈\n",
        "\n",
        "### 문항 2) 알맞은 알파벳을 \"그림\" 또는 <보기>에서 고르고 순서대로 입력하시오.\n",
        "(혼동될 수 있는 부분을 방지하기위해 여러 그림에서 동일 Latter를 표기하였고, <보기>에도 중복표현되어 있습니다. Latterd의 개수와 문항의 개수는 동일하지 않습니다.)\n",
        "\n",
        "- 정답 예시) \n",
        "   - 261305762671a58cae5b74990bcfc236c2336fb04a0fbac626166d9491d2884c\n",
        "\n",
        "  - 05db7e4163a9fd013956fb9e3ffff3fbab75331418e217158424f06400ff29b2\n",
        "\n",
        "- 정답은 대문자 Latter 10개를 [Hash256](https://emn178.github.io/online-tools/sha256.html)으로 변환하여 입력하시오. \n",
        "\n",
        "<img src=https://ds-lecture-data.s3.ap-northeast-2.amazonaws.com/etc/note_image/Quiz1.png>\n",
        "\n",
        "- (1) Input Layer:\n",
        "- (2) Hidden Layer:\n",
        "- (3) Output Layer:\n",
        "- (4) Neuron(node):\n",
        "- (5) Weight:\n",
        "- (6) Bias:\n",
        "- (7) Activation Function:\n",
        "- (8) Node Map:\n",
        "- (9) interation :\n",
        "- (10) epoch :\n",
        "\n",
        "### <보기>\n",
        "\n",
        "---\n",
        "F. 모델에서 학습해야 할 대상 중 가장 많은 양을 차지함\n",
        "\n",
        "I. 모델을 학습시키는 가장 작은 단위자 이 단위의 반복이 일어나면서 모델이 점점 좋아진다.\n",
        "\n",
        "J. 모델을 학습시키는 단위이자 모든 데이터셋을 전체적으로 이용했을 때를 기준\n",
        "\n",
        "K. 모델 속에서 일어나는 가중치의 합이 임계값 이상인지 확인함\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "- 정답 만드는 방법 - [링크](https://emn178.github.io/online-tools/sha256.html)에서 다음처럼 영어문자를 입력하면 바뀜. 하나라도 틀리면 오답으로 처리됨.\n",
        "<img src=https://ds-lecture-data.s3.ap-northeast-2.amazonaws.com/etc/note_image/SHA256.png width=500>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1XyXKEvgxD4W"
      },
      "source": [
        "# 문항 2\n",
        "# 05db7e4163a9fd013956fb9e3ffff3fbab75331418e217158424f06400ff29b2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q024AZ9hRDMl"
      },
      "source": [
        "## 3) 아래 코드에서 만들어진 네트워크의 Node Map의 형태를 표현해보시오.\n",
        "(matrix를 표현할 때, NxMxC 형태로 표기합니다.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HhBFw6Y2Qrao"
      },
      "source": [
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Flatten(input_shape=(28, 28)), # - - - (a)\n",
        "  tf.keras.layers.Dense(256, activation='relu'), # - - - (b)\n",
        "  tf.keras.layers.Dense(10, activation='softmax') # - - - (c) 10\n",
        "])"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lA3m1glVxjvq"
      },
      "source": [
        "### 문제\n",
        "\n",
        "- (1) Input layer에 100장의 이미지가 들어왔을 때 (a)를 통과하기 전의 데이터의 형태 \n",
        "\n",
        "- (2) Input layer에 100장의 이미지가 들어왔을 때 (a)를 통과한 뒤의 데이터의 형태\n",
        "\n",
        "- (3) (a)에서 (b)로 갈 때의 Weight matrix의 형태 \n",
        "\n",
        "- (4) 클래스가 1000개이면, (c)의 10은 얼마로 바뀌어야 하는가?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LX5oqUQQxQJU"
      },
      "source": [
        "# 문항 3\n",
        "(1)\n",
        "(2)\n",
        "(3)\n",
        "(4) 1000"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SvNvoQJk-xb-"
      },
      "source": [
        "## 4) 실제 데이터 과제\n",
        " - 아래 주어진 데이터를 신경망을 이용하여 Classification 문제를 풀어보세요.\n",
        " - 또한 머신러닝에서 배운 방법(배우지 않은 머신러닝 방법론(SVM 등)도 가능)을 이용하여 비교해보세요."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pqOkgM9wnmNu"
      },
      "source": [
        "입력 데이터 샘플과 특징점(features): 1077 샘플 x 69 특징점 (변수), \n",
        "mouse_protein_X.xls \n",
        "\n",
        "데이터 label: 다운증후군 (1), 정상군 (2) \n",
        "mouse_protein_label.xls\n",
        "\n",
        "데이터는 다운증후군과 정상군 마우스 피질의 핵 분획에서 검출 가능한 신호를 생성하는 69 개 단백질의 발현 수준으로 구성되어 있다. 38 마리의 대조군 마우스와 34 마리의 trisomic mice (다운 증후군)가 총 72 마리로 각각 15번의 측정치가 등록되어 있는데 그 중에서 3개의 샘플이 측정이 안되어 총 1077개의 샘플이 있다. 라벨로는 다운증후군 1, 정상군 2로 할당 되어 있다. \n",
        "\n",
        "해당데이터는 다음과 같이 볼 수 있다. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gULuO1ETO-6G"
      },
      "source": [
        "import pandas as pd\n",
        "df = pd.read_excel(\"https://ds-lecture-data.s3.ap-northeast-2.amazonaws.com/MouseProtein/mouse_protein_X.xls\", header=None)\n",
        "df_label = pd.read_excel(\"https://ds-lecture-data.s3.ap-northeast-2.amazonaws.com/MouseProtein/mouse_protein_label.xls\", header=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6I-8OQ_APLtG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "6777556b-1160-41a8-dbef-be7a419e1411"
      },
      "source": [
        "# 샘플당 100개의 특성(feature)을 가진 데이터\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>40</th>\n",
              "      <th>41</th>\n",
              "      <th>42</th>\n",
              "      <th>43</th>\n",
              "      <th>44</th>\n",
              "      <th>45</th>\n",
              "      <th>46</th>\n",
              "      <th>47</th>\n",
              "      <th>48</th>\n",
              "      <th>49</th>\n",
              "      <th>50</th>\n",
              "      <th>51</th>\n",
              "      <th>52</th>\n",
              "      <th>53</th>\n",
              "      <th>54</th>\n",
              "      <th>55</th>\n",
              "      <th>56</th>\n",
              "      <th>57</th>\n",
              "      <th>58</th>\n",
              "      <th>59</th>\n",
              "      <th>60</th>\n",
              "      <th>61</th>\n",
              "      <th>62</th>\n",
              "      <th>63</th>\n",
              "      <th>64</th>\n",
              "      <th>65</th>\n",
              "      <th>66</th>\n",
              "      <th>67</th>\n",
              "      <th>68</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.50364</td>\n",
              "      <td>0.74719</td>\n",
              "      <td>0.43018</td>\n",
              "      <td>2.8163</td>\n",
              "      <td>5.9902</td>\n",
              "      <td>0.21883</td>\n",
              "      <td>0.17757</td>\n",
              "      <td>2.3737</td>\n",
              "      <td>0.23222</td>\n",
              "      <td>1.7509</td>\n",
              "      <td>0.68791</td>\n",
              "      <td>0.30638</td>\n",
              "      <td>0.40270</td>\n",
              "      <td>0.29693</td>\n",
              "      <td>1.02210</td>\n",
              "      <td>0.60567</td>\n",
              "      <td>1.8777</td>\n",
              "      <td>2.3087</td>\n",
              "      <td>0.44160</td>\n",
              "      <td>0.85937</td>\n",
              "      <td>0.41629</td>\n",
              "      <td>0.36961</td>\n",
              "      <td>0.17894</td>\n",
              "      <td>1.8664</td>\n",
              "      <td>3.6852</td>\n",
              "      <td>1.5372</td>\n",
              "      <td>0.26453</td>\n",
              "      <td>0.31968</td>\n",
              "      <td>0.81387</td>\n",
              "      <td>0.16585</td>\n",
              "      <td>0.45391</td>\n",
              "      <td>3.0376</td>\n",
              "      <td>0.36951</td>\n",
              "      <td>0.45854</td>\n",
              "      <td>0.33534</td>\n",
              "      <td>0.82519</td>\n",
              "      <td>0.57692</td>\n",
              "      <td>0.44810</td>\n",
              "      <td>0.58627</td>\n",
              "      <td>0.39472</td>\n",
              "      <td>0.33957</td>\n",
              "      <td>0.48286</td>\n",
              "      <td>0.29417</td>\n",
              "      <td>0.18215</td>\n",
              "      <td>0.84273</td>\n",
              "      <td>0.19261</td>\n",
              "      <td>1.4431</td>\n",
              "      <td>0.29470</td>\n",
              "      <td>0.35460</td>\n",
              "      <td>1.3391</td>\n",
              "      <td>0.17012</td>\n",
              "      <td>0.15910</td>\n",
              "      <td>0.18885</td>\n",
              "      <td>0.10631</td>\n",
              "      <td>0.14499</td>\n",
              "      <td>0.17667</td>\n",
              "      <td>0.12519</td>\n",
              "      <td>0.11529</td>\n",
              "      <td>0.22804</td>\n",
              "      <td>0.14276</td>\n",
              "      <td>0.43096</td>\n",
              "      <td>0.24754</td>\n",
              "      <td>1.6033</td>\n",
              "      <td>2.0149</td>\n",
              "      <td>0.10823</td>\n",
              "      <td>1.04500</td>\n",
              "      <td>0.83156</td>\n",
              "      <td>0.18885</td>\n",
              "      <td>1.6757</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.51462</td>\n",
              "      <td>0.68906</td>\n",
              "      <td>0.41177</td>\n",
              "      <td>2.7895</td>\n",
              "      <td>5.6850</td>\n",
              "      <td>0.21164</td>\n",
              "      <td>0.17282</td>\n",
              "      <td>2.2921</td>\n",
              "      <td>0.22697</td>\n",
              "      <td>1.5964</td>\n",
              "      <td>0.69501</td>\n",
              "      <td>0.29905</td>\n",
              "      <td>0.38599</td>\n",
              "      <td>0.28132</td>\n",
              "      <td>0.95668</td>\n",
              "      <td>0.58756</td>\n",
              "      <td>1.7258</td>\n",
              "      <td>2.0430</td>\n",
              "      <td>0.44522</td>\n",
              "      <td>0.83466</td>\n",
              "      <td>0.40036</td>\n",
              "      <td>0.35618</td>\n",
              "      <td>0.17368</td>\n",
              "      <td>1.7610</td>\n",
              "      <td>3.4853</td>\n",
              "      <td>1.5092</td>\n",
              "      <td>0.25573</td>\n",
              "      <td>0.30442</td>\n",
              "      <td>0.78050</td>\n",
              "      <td>0.15719</td>\n",
              "      <td>0.43094</td>\n",
              "      <td>2.9219</td>\n",
              "      <td>0.34228</td>\n",
              "      <td>0.42356</td>\n",
              "      <td>0.32483</td>\n",
              "      <td>0.76172</td>\n",
              "      <td>0.54510</td>\n",
              "      <td>0.42088</td>\n",
              "      <td>0.54510</td>\n",
              "      <td>0.36825</td>\n",
              "      <td>0.32196</td>\n",
              "      <td>0.45452</td>\n",
              "      <td>0.27643</td>\n",
              "      <td>0.18209</td>\n",
              "      <td>0.84761</td>\n",
              "      <td>0.19482</td>\n",
              "      <td>1.4395</td>\n",
              "      <td>0.29406</td>\n",
              "      <td>0.35455</td>\n",
              "      <td>1.3063</td>\n",
              "      <td>0.17143</td>\n",
              "      <td>0.15813</td>\n",
              "      <td>0.18457</td>\n",
              "      <td>0.10659</td>\n",
              "      <td>0.15047</td>\n",
              "      <td>0.17831</td>\n",
              "      <td>0.13428</td>\n",
              "      <td>0.11823</td>\n",
              "      <td>0.23807</td>\n",
              "      <td>0.14204</td>\n",
              "      <td>0.45716</td>\n",
              "      <td>0.25763</td>\n",
              "      <td>1.6717</td>\n",
              "      <td>2.0046</td>\n",
              "      <td>0.10975</td>\n",
              "      <td>1.00990</td>\n",
              "      <td>0.84927</td>\n",
              "      <td>0.20040</td>\n",
              "      <td>1.7436</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.50918</td>\n",
              "      <td>0.73025</td>\n",
              "      <td>0.41831</td>\n",
              "      <td>2.6872</td>\n",
              "      <td>5.6221</td>\n",
              "      <td>0.20901</td>\n",
              "      <td>0.17572</td>\n",
              "      <td>2.2833</td>\n",
              "      <td>0.23025</td>\n",
              "      <td>1.5613</td>\n",
              "      <td>0.67735</td>\n",
              "      <td>0.29128</td>\n",
              "      <td>0.38100</td>\n",
              "      <td>0.28171</td>\n",
              "      <td>1.00360</td>\n",
              "      <td>0.60245</td>\n",
              "      <td>1.7319</td>\n",
              "      <td>2.0180</td>\n",
              "      <td>0.46767</td>\n",
              "      <td>0.81433</td>\n",
              "      <td>0.39985</td>\n",
              "      <td>0.36809</td>\n",
              "      <td>0.17390</td>\n",
              "      <td>1.7655</td>\n",
              "      <td>3.5715</td>\n",
              "      <td>1.5012</td>\n",
              "      <td>0.25961</td>\n",
              "      <td>0.31175</td>\n",
              "      <td>0.78515</td>\n",
              "      <td>0.16090</td>\n",
              "      <td>0.42319</td>\n",
              "      <td>2.9441</td>\n",
              "      <td>0.34370</td>\n",
              "      <td>0.42500</td>\n",
              "      <td>0.32485</td>\n",
              "      <td>0.75703</td>\n",
              "      <td>0.54362</td>\n",
              "      <td>0.40463</td>\n",
              "      <td>0.55299</td>\n",
              "      <td>0.36388</td>\n",
              "      <td>0.31309</td>\n",
              "      <td>0.44720</td>\n",
              "      <td>0.25665</td>\n",
              "      <td>0.18439</td>\n",
              "      <td>0.85617</td>\n",
              "      <td>0.20074</td>\n",
              "      <td>1.5244</td>\n",
              "      <td>0.30188</td>\n",
              "      <td>0.38609</td>\n",
              "      <td>1.2796</td>\n",
              "      <td>0.18546</td>\n",
              "      <td>0.14870</td>\n",
              "      <td>0.19053</td>\n",
              "      <td>0.10830</td>\n",
              "      <td>0.14533</td>\n",
              "      <td>0.17621</td>\n",
              "      <td>0.13256</td>\n",
              "      <td>0.11776</td>\n",
              "      <td>0.24482</td>\n",
              "      <td>0.14244</td>\n",
              "      <td>0.51047</td>\n",
              "      <td>0.25534</td>\n",
              "      <td>1.6635</td>\n",
              "      <td>2.0168</td>\n",
              "      <td>0.10820</td>\n",
              "      <td>0.99685</td>\n",
              "      <td>0.84671</td>\n",
              "      <td>0.19368</td>\n",
              "      <td>1.9264</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.44211</td>\n",
              "      <td>0.61708</td>\n",
              "      <td>0.35863</td>\n",
              "      <td>2.4669</td>\n",
              "      <td>4.9795</td>\n",
              "      <td>0.22289</td>\n",
              "      <td>0.17646</td>\n",
              "      <td>2.1523</td>\n",
              "      <td>0.20700</td>\n",
              "      <td>1.5951</td>\n",
              "      <td>0.58328</td>\n",
              "      <td>0.29673</td>\n",
              "      <td>0.37709</td>\n",
              "      <td>0.31383</td>\n",
              "      <td>0.87539</td>\n",
              "      <td>0.52029</td>\n",
              "      <td>1.5669</td>\n",
              "      <td>2.1328</td>\n",
              "      <td>0.47767</td>\n",
              "      <td>0.72770</td>\n",
              "      <td>0.38564</td>\n",
              "      <td>0.36297</td>\n",
              "      <td>0.17945</td>\n",
              "      <td>1.2863</td>\n",
              "      <td>2.9701</td>\n",
              "      <td>1.4197</td>\n",
              "      <td>0.25954</td>\n",
              "      <td>0.27922</td>\n",
              "      <td>0.73449</td>\n",
              "      <td>0.16221</td>\n",
              "      <td>0.41061</td>\n",
              "      <td>2.5002</td>\n",
              "      <td>0.34451</td>\n",
              "      <td>0.42921</td>\n",
              "      <td>0.33012</td>\n",
              "      <td>0.74698</td>\n",
              "      <td>0.54676</td>\n",
              "      <td>0.38686</td>\n",
              "      <td>0.54785</td>\n",
              "      <td>0.36677</td>\n",
              "      <td>0.32849</td>\n",
              "      <td>0.44265</td>\n",
              "      <td>0.39853</td>\n",
              "      <td>0.16177</td>\n",
              "      <td>0.76023</td>\n",
              "      <td>0.18417</td>\n",
              "      <td>1.6124</td>\n",
              "      <td>0.29638</td>\n",
              "      <td>0.29068</td>\n",
              "      <td>1.1988</td>\n",
              "      <td>0.15980</td>\n",
              "      <td>0.16611</td>\n",
              "      <td>0.18532</td>\n",
              "      <td>0.10318</td>\n",
              "      <td>0.14066</td>\n",
              "      <td>0.16380</td>\n",
              "      <td>0.12321</td>\n",
              "      <td>0.11744</td>\n",
              "      <td>0.23495</td>\n",
              "      <td>0.14507</td>\n",
              "      <td>0.43100</td>\n",
              "      <td>0.25110</td>\n",
              "      <td>1.4846</td>\n",
              "      <td>1.9572</td>\n",
              "      <td>0.11988</td>\n",
              "      <td>0.99022</td>\n",
              "      <td>0.83328</td>\n",
              "      <td>0.19211</td>\n",
              "      <td>1.7006</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.43494</td>\n",
              "      <td>0.61743</td>\n",
              "      <td>0.35880</td>\n",
              "      <td>2.3658</td>\n",
              "      <td>4.7187</td>\n",
              "      <td>0.21311</td>\n",
              "      <td>0.17363</td>\n",
              "      <td>2.1340</td>\n",
              "      <td>0.19216</td>\n",
              "      <td>1.5042</td>\n",
              "      <td>0.55096</td>\n",
              "      <td>0.28696</td>\n",
              "      <td>0.36350</td>\n",
              "      <td>0.27796</td>\n",
              "      <td>0.86491</td>\n",
              "      <td>0.50799</td>\n",
              "      <td>1.4801</td>\n",
              "      <td>2.0137</td>\n",
              "      <td>0.48342</td>\n",
              "      <td>0.68779</td>\n",
              "      <td>0.36753</td>\n",
              "      <td>0.35531</td>\n",
              "      <td>0.17484</td>\n",
              "      <td>1.3247</td>\n",
              "      <td>2.8963</td>\n",
              "      <td>1.3599</td>\n",
              "      <td>0.25070</td>\n",
              "      <td>0.27367</td>\n",
              "      <td>0.70270</td>\n",
              "      <td>0.15483</td>\n",
              "      <td>0.39855</td>\n",
              "      <td>2.4566</td>\n",
              "      <td>0.32913</td>\n",
              "      <td>0.40876</td>\n",
              "      <td>0.31341</td>\n",
              "      <td>0.69196</td>\n",
              "      <td>0.53686</td>\n",
              "      <td>0.36082</td>\n",
              "      <td>0.51282</td>\n",
              "      <td>0.35155</td>\n",
              "      <td>0.31221</td>\n",
              "      <td>0.41909</td>\n",
              "      <td>0.39345</td>\n",
              "      <td>0.16020</td>\n",
              "      <td>0.76811</td>\n",
              "      <td>0.18572</td>\n",
              "      <td>1.6458</td>\n",
              "      <td>0.29683</td>\n",
              "      <td>0.30935</td>\n",
              "      <td>1.2070</td>\n",
              "      <td>0.16465</td>\n",
              "      <td>0.16069</td>\n",
              "      <td>0.18822</td>\n",
              "      <td>0.10478</td>\n",
              "      <td>0.14198</td>\n",
              "      <td>0.16771</td>\n",
              "      <td>0.13684</td>\n",
              "      <td>0.11605</td>\n",
              "      <td>0.25553</td>\n",
              "      <td>0.14087</td>\n",
              "      <td>0.48123</td>\n",
              "      <td>0.25177</td>\n",
              "      <td>1.5348</td>\n",
              "      <td>2.0091</td>\n",
              "      <td>0.11952</td>\n",
              "      <td>0.99777</td>\n",
              "      <td>0.87867</td>\n",
              "      <td>0.20560</td>\n",
              "      <td>1.8397</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        0        1        2       3   ...       65       66       67      68\n",
              "0  0.50364  0.74719  0.43018  2.8163  ...  1.04500  0.83156  0.18885  1.6757\n",
              "1  0.51462  0.68906  0.41177  2.7895  ...  1.00990  0.84927  0.20040  1.7436\n",
              "2  0.50918  0.73025  0.41831  2.6872  ...  0.99685  0.84671  0.19368  1.9264\n",
              "3  0.44211  0.61708  0.35863  2.4669  ...  0.99022  0.83328  0.19211  1.7006\n",
              "4  0.43494  0.61743  0.35880  2.3658  ...  0.99777  0.87867  0.20560  1.8397\n",
              "\n",
              "[5 rows x 69 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zrIvw52rPdx4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1619c90a-16a0-4386-ea41-cd2649ea8754"
      },
      "source": [
        "print(df_label.head())\n",
        "print(df_label.tail())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   0\n",
            "0  1\n",
            "1  1\n",
            "2  1\n",
            "3  1\n",
            "4  1\n",
            "      0\n",
            "1072  2\n",
            "1073  2\n",
            "1074  2\n",
            "1075  2\n",
            "1076  2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yilj1IY3M4Zr"
      },
      "source": [
        "---\n",
        "\n",
        "4-1. 사용한 모델을 입력합니다. \n",
        "\n",
        "4-2. Accuracy를 입력합니다. \n",
        "\n",
        "4-3. Precision 을 입력합니다. \n",
        "\n",
        "4-4. Recall 을 입력합니다.\n",
        "\n",
        "4-5. F1 score 를 입력합니다. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JvI4eF8uniKH"
      },
      "source": [
        "# (도전과제 1) Tensorflow 예제에서 사용해보았던 adam optimizer 조사과제\n",
        "- ADAM 옵티마이저에 대해서 조사해보고 기본적인 경사하강법(GD)과의 차이점, 다른 옵티마이저와의 차이점을 블로그에 작성해보고, 공유합니다. 공유된 자료가 있다면, 내 자료와 비교하면서 어느부분에서 차이가 있는 지 꼭 확인합니다. \n",
        "\n",
        "# (도전과제 2) Tensorflow 를 이용하여 나만의 과제 수행하기\n",
        "\n",
        "- 나만의 파이프라인을 제작합니다. \n",
        "\n",
        "- 공개된 데이터를 찾아 모델을 학습시켜봅니다. \n",
        "\n",
        "- 모델을 여러개 제작하여 성능을 비교해봅니다. \n",
        "\n",
        "- 이 과정에서 얻은 인사이트를 작성해봅니다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NiyAhPQfN-PI"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}